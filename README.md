# bushchat

A non-linear chat interface for LLMs. Conversations are displayed as a tree structure, allowing you to branch off at any point and explore multiple conversation paths simultaneously.

![bushchat screenshot](doc/sc.png)

## Features

- ğŸŒ³ **Tree-based conversations** - Branch anywhere, not linear
- ğŸ”€ **Merge branches** - Combine conversation paths with configurable context
- ğŸ—‚ï¸ **Artifacts** - Add text or images to the canvas and merge them into conversations
- ğŸ–¼ï¸ **Multimodal support** - Vision models can process images (auto-detected from model capabilities, at least works with Openrouter)
- âœï¸ **Edit propagation** - Edit any node and all descendants automatically regenerate
- ğŸ”Œ **Any OpenAI-compatible provider** - OpenRouter, OpenAI, Ollama, LM Studio, etc.
- ğŸ’¾ **Local storage** - Everything persists only in your browser
- ğŸ”— **Stateless sharing** - Entire conversation trees are compressed into a sharable URL. (Links can get huge)

## Setup

1. Clone the repo
2. `npm install`
3. `npm run dev`

## Configuration

**Option A: Server-side (`.env` file)**

```
OPENAI_API_KEY=your-key-here
```

**Option B: Client-side (Settings panel)**

- Click the âš™ï¸ icon in the top-left panel
- Enter your API key and optionally a custom API URL
- Use the ğŸ™ˆ checkbox to save the API key in browser storage

For local LLMs, set the API URL to your local endpoint (e.g., `http://localhost:11434/v1` for Ollama).
